\chapter{Recursion}


Recursion is an important idea across computer science.
For us, in learning data structures and basic algorithms, there are several ways to both think about and use recursion.
At its simplest, one can think of \textbf{recursion} as: \emph{when a function calls itself}.


Although true, this definition doesn't really tell us how or why to use it, and what to do with it when we find it in the wild.
Our objectives for this chapter are three-fold:

\begin{enumerate}

\item 

Understand how recursion is implemented in a computer and translate this into a model for how to think about recursive functions.



\item 

Use recursion as a problem solving technique, recognizing the role of "subproblems" as a primary motivation for recursion.



\item 

Be able to analyze the running time of recursive functions.



\end{enumerate}

Here is an example of a recursive function.

\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{k} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{k}{return} \PY{n}{f}\PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{k}
    \PY{k}{return} \PY{l+m+mi}{0}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}



This is actually just our familiar \texttt{sumk} function in disguise.
How does it work?
To find the sum of numbers from $1$ to $k$, it just finds the sum of the numbers from $1$ to $k-1$ and then adds $k$.
Yes, we know there is a better way to compute this sum, but there is something satisfying about this approach.
We were asked to solve a problem given some input $k$ and we pretended we already had a solution for smaller numbers ($k-1$ in this case).
Then, we used that solution to construct a solution for $k$.
The magic comes from the fact that the "pretended" solution was actually the same function.
I call this the \emph{pretend someone else wrote it} approach to recursion.
It's very handy for both writing and analyzing recursive functions.

\section{Recursion and Induction}


The function \texttt{f} above does the same thing as our \texttt{sumk} function that we saw earlier.
You can check it using something called induction.
In fact, this checking process is exactly what it means to "do a proof by induction".
There, you prove a fact, by checking that it is true in the base case.
Then, you prove that it's true for \texttt{k} assuming it's true for \texttt{k-1}.
In the special case of checking that our function \texttt{f} is identical to the \texttt{sumk} function that simply returns $\frac{k(k+1)}{2}$, it looks as follows.
First, check that indeed


\[
  f(0) = 0 = \frac{0 * (0 + 1)}{2}.
\]


Then observe that


\[
    f(k) = f(k-1) + k = \frac{(k-1)(k-1 + 1)}{2} + k = \frac{(k-1)k + 2k}{2} = \frac{k(k+1)}{2}.
\]


We won't get many opportunities to do such proofs in this class, but it's worth remembering that there is a strong connection between recursion and induction and working through this connection can enrich your understanding of both concepts.

\section{Some Basics}


Here are some basic rules that you should try to follow to make sure your recursive algorithms terminate.

\begin{enumerate}

\item Have a base case.  If every function call makes another recursive call, you will never terminate.

\item Recursive calls should move towards the base case.  This usually means that the recursive calls are made on "smaller" subproblems.  Here, "smaller" can mean different things.

\end{enumerate}

It's not hard to write a recursive function that never stops \emph{in theory}.
However, in practice, so-called infinite recursion is pretty quickly discovered with a \texttt{RecursionError}.
Python has a limit on the recursion depth.
This limit is usually around 1000.

\section{The Function Call Stack}


To think clearly about recursion, it helps to have an idea of how recursion works on a real computer.
It is easiest to first understand how all function calls work and then it should be clear that there is no technical difference in making a recursive function call compared to any other function call.


\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{n}{var} \PY{o}{=} \PY{n}{k} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
    \PY{k}{return} \PY{n}{g}\PY{p}{(}\PY{n}{k}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{var}

\PY{k}{def} \PY{n+nf}{g}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{n}{var} \PY{o}{=} \PY{n}{k} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n}{var} \PY{o}{+} \PY{l+m+mi}{1}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}
15
\end{Verbatim}


The following code gives a \texttt{RecursionError} even though none of the functions call themselves directly.
The error is really just signaling that the call stack reached its limit.

\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{a}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{k} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:} \PY{k}{return} \PY{l+m+mi}{0}
    \PY{k}{return} \PY{n}{b}\PY{p}{(}\PY{n}{k}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{b}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{c}\PY{p}{(}\PY{n}{k}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{c}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{a}\PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{a}\PY{p}{(}\PY{l+m+mi}{340}\PY{p}{)}
\end{Verbatim}



An interesting recursive example con be constructed by creating two lists, each one containing the other.


\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{A} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
\PY{n}{B} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
\PY{n}{A}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{A}\PY{p}{)}
\PY{n}{B}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{B}\PY{p}{)}
\PY{n}{A} \PY{o}{==} \PY{n}{B}
\end{Verbatim}

\begin{Verbatim}
\end{Verbatim}


In this case, the recursive function is \texttt{list.\_\_eq\_\_}, the method that compares two lists when you use \texttt{==}.  It compares the list by checking if the elements are equal.  The lists \texttt{A} and \texttt{B} each have length 2.  The first elements match.  The second element of each list is another list.  To compare them, there is another call to \texttt{list.\_\_eq\_\_}.  This is a repeat of the first call and the process repeats until the recursion limit is reached.

\section{The Fibonacci Sequence}


Here is a classic example of a recursively defined function.
It was originally named for Leonardo Fibonacci who studied it in the context of predicting the growth of rabbit populations.


\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{fib}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{k} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{:} \PY{k}{return} \PY{n}{k}
    \PY{k}{return} \PY{n}{fib}\PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{fib}\PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{p}{[}\PY{n}{fib}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]
\end{Verbatim}


This works, but it starts to get really slow, even for small values of \texttt{k}.
For example, I tried to run it for \texttt{k = 40} and gave up on it.


This is a case that we will encounter many times in this course.
Once we embrace recursion as a way to think about breaking problems into smaller pieces, we may find very short recursive algorithms, but for one reason or another, we may want to rewrite them without recursion.
Here's a version that uses a loop instead.


\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{fib}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{n}{a}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
        \PY{n}{a}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{b}\PY{p}{,} \PY{n}{a} \PY{o}{+} \PY{n}{b}
    \PY{k}{return} \PY{n}{a}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{fib}\PY{p}{(}\PY{l+m+mi}{400}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}
176023680645013966468226945392411250770384383304492191886725992896575345044216019675
\end{Verbatim}


This is much better!
At first look, it might seem like the only difference in the total work is just in the extra overhead of making function calls rather than updating local variables.
This is wrong.
Let's work out in more detail exactly what function calls are made in the recursive implementation.
Let's say we called \texttt{fib(6)}.
This makes calls to \texttt{fib(5)} and \texttt{fib(4)}.
This, in turn makes calls to \texttt{fib(4)} and \texttt{fib(3)} as well as \texttt{fib(3)} and \texttt{fib(2)}.
Each of these four function calls will make two more function calls each.
We can draw them out in a tree.
Notice, that already, we have multiple calls to \texttt{fib(4)} as well as to \texttt{fib(3)}.
We might as well ask, how many times will we call the same function with the same value?
If we compute \texttt{fib(k)}, the answer, interestingly enough is related to the Fibonacci numbers themselves.
Let $T(k)$ denote the number of calls to \texttt{fib} when computing \texttt{fib(k)}.
It's easy to see that $T(k) = T(k-1) + T(k-2) + 1$.
In this case, the result is nearly exactly the Fibonacci numbers ($T(1) = 1$, $T(2) = 2$, $T(3) = 4$, $T(4) = 7$, ...).  In each case, the value of $T$ is one less than a Fibonacci number.  Thus, the running time of \texttt{fib} will grow like the Fibonacci numbers, or equivalently, it will grow like ideal rabbit families, exponentially.  The $k$th Fibonacci number is about $\phi^k$, where $\phi = \frac{1+\sqrt{5}}{2}\sim1.618$ is known as the Golden Ratio.

\section{Euclid's Algorithm}


Euclid's algorithm is a classic in every sense of the word.
The input is a pair of integers \texttt{a, b} and the output is the greatest common divisor, i.e., the largest integer that divides both \texttt{a} and \texttt{b} evenly.
It is a very simple recursive algorithm.
We'll look at the code first and then try to figure out what its doing, why it works, and how we can improve it.

\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{gcd}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{a} \PY{o}{==} \PY{n}{b}\PY{p}{:}
        \PY{k}{return} \PY{n}{a}
    \PY{k}{if} \PY{n}{a} \PY{o}{\PYZgt{}} \PY{n}{b}\PY{p}{:}
        \PY{n}{a}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{b}\PY{p}{,} \PY{n}{a}
    \PY{k}{return} \PY{n}{gcd}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b} \PY{o}{\PYZhy{}} \PY{n}{a}\PY{p}{)}
\end{Verbatim}



Like all recursive algorithms, there is a base case.
Here, if \texttt{a == b}, then \texttt{a} (or equivalently, \texttt{b}) is the answer and we return it.
Otherwise, we arrange it so \texttt{a < b} and make a recursive call: \texttt{gcd(a, b - a)}.


When we walk through some examples, by hand, it seems like the algorithm makes many recursive calls when \texttt{b} is much bigger than \texttt{a}.
In fact, it's not hard to see that we'll need at least \texttt{a // b} recursive calls before we swap \texttt{a} and \texttt{b}.
These calls are just repeatedly subtracting the same number until it gets sufficiently small.
This is known to elementary school students as division.
This is actually a deep idea that's worth pondering.  Division is iterated subtraction the same way that multiplication is iterated addition, exponentiation is iterated multiplication, and logarithms are iterated division.
Theoretical computer scientists even have a use for iterated logarithms (we call it $\log^\star$, pronounced "log star").


We can just do the division directly rather than repeatedly subtracting the smaller from the bigger.
The result is a slight change to the base case and replacing the subtraction with the modulus operation.
Here is the revised code.


\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{gcd}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{a} \PY{o}{\PYZgt{}} \PY{n}{b}\PY{p}{:}
        \PY{n}{a}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{b}\PY{p}{,} \PY{n}{a}
    \PY{k}{if} \PY{n}{a} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{k}{return} \PY{n}{b}
    \PY{k}{return} \PY{n}{gcd}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b} \PY{o}{\PYZpc{}} \PY{n}{a}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GCD of 12 and 513 is}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{gcd}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{513}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GCD of 19 and 513 is}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{gcd}\PY{p}{(}\PY{l+m+mi}{19}\PY{p}{,} \PY{l+m+mi}{513}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GCD of 19 and 515 is}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{gcd}\PY{p}{(}\PY{l+m+mi}{515} \PY{p}{,}\PY{l+m+mi}{19}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}
GCD of 12 and 513 is 3
GCD of 19 and 513 is 19
GCD of 19 and 515 is 1
\end{Verbatim}


Incidentally, if \texttt{a} and \texttt{b} are allowed to be arbitrary numbers, you might try find examples where the gcd algorithm gets caught in an infinite recursion.  In this way, you might discover the irrational numbers.  If \texttt{a} and \texttt{b} are rational, then the algorithm is also guaranteed to terminate.


If you wanted to find an example that was as bad as possible, you might try to find a pair \texttt{(a,b)} such that after one iteration, you get the pair \texttt{(b-a, a)} where the ratio of the numbers is the same.  Then, you can check that this will continue and therefore, you'll never get closer to that base case.  But is that possible?  Is there a pair of numbers with this property?  The answer is yes.  One could use $a = 1$ and $b = \phi$, the Golden Ratio.

